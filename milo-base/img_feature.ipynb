{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuktabagdi/miniconda3/envs/feature-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import random\n",
    "# from submodlib import FacilityLocationFunction\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 5.16k/5.16k [00:00<00:00, 12.2MB/s]\n",
      "Downloading data: 100%|██████████| 120M/120M [00:23<00:00, 5.15MB/s] \n",
      "Downloading data: 100%|██████████| 23.9M/23.9M [00:04<00:00, 4.85MB/s]\n",
      "Generating train split: 100%|██████████| 50000/50000 [00:00<00:00, 518407.96 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 625548.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "cifar10 = load_dataset(\"cifar10\")\n",
    "\n",
    "# Define the image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img, resnet):\n",
    "    \"\"\"\n",
    "    :param img: A CIFAR image\n",
    "    :return: List of features\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply the transformation and convert the image to a tensor\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "    # Extract the features using the ResNet18 model\n",
    "    with torch.no_grad():\n",
    "        features = resnet(img_tensor)\n",
    "\n",
    "    # Flatten the features and convert to a 1D numpy array\n",
    "    features = features.squeeze().numpy()\n",
    "    features = features.flatten()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a threading.Event object\n",
    "event = threading.Event()\n",
    "\n",
    "def extract_features_resnet_threaded_cifar(cifar_dataset, num_threads=4 ):\n",
    "    num_imgs = len(cifar_dataset)\n",
    "\n",
    "    # Create a queue to store the image data\n",
    "    img_queue = queue.Queue()\n",
    "\n",
    "    # Create a list to store the extracted features\n",
    "    features_list = []\n",
    "    label_list = []\n",
    "    index_list = []\n",
    "\n",
    "    # Create a lock to protect the features list\n",
    "    features_lock = threading.Lock()\n",
    "\n",
    "    # Create a list of threads\n",
    "    threads = []\n",
    "    models = []\n",
    "\n",
    "    # create multiple copies of the models for extracting features\n",
    "    for i in range(num_threads):\n",
    "        models.append(torch.hub.load('pytorch/vision:v0.11.3', 'resnet18', pretrained=True))\n",
    "\n",
    "    # Start the threads\n",
    "    for i in range(num_threads):\n",
    "        thread = threading.Thread(target=extract_features_threaded_worker, args=(img_queue, index_list, features_list, label_list, features_lock, models[i], event))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Enqueue all the image data\n",
    "    for i, img in tqdm(enumerate(cifar_dataset)):\n",
    "        img_queue.put((i, img[\"img\"], img[\"label\"]))\n",
    "\n",
    "    # Signal to the threads that all of the images have been enqueued\n",
    "    event.set()\n",
    "\n",
    "    # Wait for all of the threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    # Create a DataFrame from the extracted features\n",
    "    data = {'Index': index_list, 'Label': label_list, 'Features': features_list}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_threaded_worker(img_queue, index_list, features_list, label_list, features_lock, model, event):\n",
    "    processed_images = 0\n",
    "    while True:\n",
    "        # Get an image path from the queue\n",
    "        if img_queue.empty():\n",
    "            # Wait for the main thread to signal that all of the images have been enqueued\n",
    "            event.wait()\n",
    "\n",
    "            # If the queue is still empty, break out of the loop\n",
    "            if img_queue.empty():\n",
    "                break\n",
    "\n",
    "        index, img_path, label = img_queue.get()\n",
    "\n",
    "        # Extract the features from the image\n",
    "        img_features = extract_features(img_path, model)\n",
    "\n",
    "        # Acquire the lock\n",
    "        features_lock.acquire()\n",
    "\n",
    "        # Add the extracted features to the list\n",
    "        features_list.append(img_features)\n",
    "        index_list.append(index)\n",
    "        label_list.append(label)\n",
    "\n",
    "        # Release the lock\n",
    "        features_lock.release()\n",
    "        \n",
    "        # Increment the number of processed images\n",
    "        processed_images+=1\n",
    "\n",
    "        # If the thread has processed 1000 images, print the thread ID and the number of processed images\n",
    "        if processed_images % 1000 == 0:\n",
    "            print(f\"Thread ID: {threading.current_thread().ident} | Processed images: {processed_images}\")\n",
    "\n",
    "        # If the queue is empty, break out of the loop\n",
    "        if img_queue.empty():\n",
    "            print(f\"Thread ID: {threading.current_thread().ident} | Processed images: {processed_images}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dataset(dataset, model):\n",
    "    num_imgs = len(dataset)\n",
    "\n",
    "    # Create a list to store the extracted features\n",
    "    features_list = []\n",
    "    label_list = []\n",
    "    index_list = []\n",
    "\n",
    "    # Enqueue all the image data\n",
    "    for i, img in tqdm(enumerate(dataset)):\n",
    "        features = extract_features(img[\"img\"], model)\n",
    "        features_list.append(features)\n",
    "        label_list.append(img[\"label\"])\n",
    "        index_list.append(i)\n",
    "        if i==5:\n",
    "            break\n",
    "        if i%100==0:\n",
    "            print(time.strftime('%X'))\n",
    "\n",
    "    # Create a DataFrame from the extracted features\n",
    "    data = {'Index': index_list, 'Label': label_list, 'Features': features_list}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load the ResNet18 model\n",
    "    resnet = torch.hub.load('pytorch/vision:v0.11.3', 'resnet18', pretrained=True)\n",
    "    cifar_dataset = cifar10[\"train\"]\n",
    "    df = get_features_dataset(cifar_dataset, resnet)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yuktabagdi/.cache/torch/hub/pytorch_vision_v0.11.3\n",
      "/Users/yuktabagdi/miniconda3/envs/feature-env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yuktabagdi/miniconda3/envs/feature-env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /Users/yuktabagdi/.cache/torch/hub/pytorch_vision_v0.11.3\n",
      "Using cache found in /Users/yuktabagdi/.cache/torch/hub/pytorch_vision_v0.11.3\n",
      "Using cache found in /Users/yuktabagdi/.cache/torch/hub/pytorch_vision_v0.11.3\n",
      "Using cache found in /Users/yuktabagdi/.cache/torch/hub/pytorch_vision_v0.11.3\n",
      "50000it [00:04, 12357.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread ID: 11171262464 | Processed images: 1000\n",
      "Thread ID: 11137609728 | Processed images: 1000\n",
      "Thread ID: 11154436096 | Processed images: 1000\n",
      "Thread ID: 11120783360 | Processed images: 1000\n",
      "Thread ID: 11171262464 | Processed images: 2000\n",
      "Thread ID: 11154436096 | Processed images: 2000\n",
      "Thread ID: 11120783360 | Processed images: 2000\n",
      "Thread ID: 11137609728 | Processed images: 2000\n",
      "Thread ID: 11171262464 | Processed images: 3000\n",
      "Thread ID: 11154436096 | Processed images: 3000\n",
      "Thread ID: 11137609728 | Processed images: 3000\n",
      "Thread ID: 11120783360 | Processed images: 3000\n",
      "Thread ID: 11171262464 | Processed images: 4000\n",
      "Thread ID: 11154436096 | Processed images: 4000\n",
      "Thread ID: 11120783360 | Processed images: 4000\n",
      "Thread ID: 11137609728 | Processed images: 4000\n",
      "Thread ID: 11171262464 | Processed images: 5000\n",
      "Thread ID: 11154436096 | Processed images: 5000\n",
      "Thread ID: 11120783360 | Processed images: 5000\n",
      "Thread ID: 11137609728 | Processed images: 5000\n",
      "Thread ID: 11154436096 | Processed images: 6000\n",
      "Thread ID: 11171262464 | Processed images: 6000\n",
      "Thread ID: 11120783360 | Processed images: 6000\n",
      "Thread ID: 11137609728 | Processed images: 6000\n",
      "Thread ID: 11154436096 | Processed images: 7000\n",
      "Thread ID: 11171262464 | Processed images: 7000\n",
      "Thread ID: 11137609728 | Processed images: 7000\n",
      "Thread ID: 11120783360 | Processed images: 7000\n",
      "Thread ID: 11137609728 | Processed images: 8000\n",
      "Thread ID: 11154436096 | Processed images: 8000\n",
      "Thread ID: 11120783360 | Processed images: 8000\n",
      "Thread ID: 11171262464 | Processed images: 8000\n",
      "Thread ID: 11137609728 | Processed images: 9000\n",
      "Thread ID: 11154436096 | Processed images: 9000\n",
      "Thread ID: 11171262464 | Processed images: 9000Thread ID: 11120783360 | Processed images: 9000\n",
      "\n",
      "Thread ID: 11137609728 | Processed images: 10000\n",
      "Thread ID: 11154436096 | Processed images: 10000\n",
      "Thread ID: 11120783360 | Processed images: 10000\n",
      "Thread ID: 11171262464 | Processed images: 10000\n",
      "Thread ID: 11137609728 | Processed images: 11000\n",
      "Thread ID: 11120783360 | Processed images: 11000\n",
      "Thread ID: 11154436096 | Processed images: 11000\n",
      "Thread ID: 11171262464 | Processed images: 11000\n",
      "Thread ID: 11137609728 | Processed images: 12000\n",
      "Thread ID: 11120783360 | Processed images: 12000\n",
      "Thread ID: 11171262464 | Processed images: 12000\n",
      "Thread ID: 11154436096 | Processed images: 12000\n",
      "Thread ID: 11154436096 | Processed images: 12495\n",
      "Thread ID: 11171262464 | Processed images: 12498\n",
      "Thread ID: 11120783360 | Processed images: 12503\n",
      "Thread ID: 11137609728 | Processed images: 12504\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet18 model\n",
    "resnet = torch.hub.load('pytorch/vision:v0.11.3', 'resnet18', pretrained=True)\n",
    "\n",
    "df = extract_features_resnet_threaded_cifar(cifar10[\"train\"])\n",
    "# sort the dataframe by index\n",
    "df = df.sort_values(by='Index')\n",
    "# Convert the 'features' column to a NumPy array\n",
    "# df['Features'] = np.array(df['Features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df[\"Features\"][0]))\n",
    "import pickle\n",
    "\n",
    "with open(\"dataframe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataframe.pkl\", \"rb\") as f:\n",
    "    loaded_df = pickle.load(f)\n",
    "\n",
    "print(type(loaded_df[\"Features\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features DataFrame to a CSV file\n",
    "filename = \"features_cifar_check.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "np_array = np.array(df['Features'])\n",
    "flattened_array = np_array.tolist()\n",
    "# Create a DataFrame from the flattened array\n",
    "df = pd.DataFrame(flattened_array)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('np.csv', index=False, header=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-env",
   "language": "python",
   "name": "feature-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
