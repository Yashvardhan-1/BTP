{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f716d7ea0f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "#from torchvision.models.resnet import ResNet18_Weights\n",
    "import pickle\n",
    "import random\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:5\" # change the available gpu number\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 datasets\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "subset_fraction = 0.1\n",
    "num_runs = 1\n",
    "split_ratio = 0.9\n",
    "epochs = 100\n",
    "model_name = \"LeNet\"\n",
    "submod_func = \"facility-location\"\n",
    "optimizer_name = \"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LeNet_model import LeNet\n",
    "from models.resent_models import get_resent101_model, get_resent18_model\n",
    "\n",
    "model = LeNet(out_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.milo.subset_sampler import RandomSubsetSampler\n",
    "from utils.milo.subset_dataset import SubDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_name==\"SGD_ann\":\n",
    "    optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "elif optimizer_name==\"adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/ganesh/namitha/yash/BTP/neural-submod\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/seq_submod_subset/permutation_subsets/facility-location_disparity-sum_disparity-min_graph-cut.pkl\", \"rb\") as f:\n",
    "#     test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = \"./data/seq_submod_subset/permutation_subsets_4/\"\n",
    "dir_list = os.listdir(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disparity-min_graph-cut_facility-location_disparity-sum.pkl\n"
     ]
    }
   ],
   "source": [
    "print(dir_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading disparity-min_graph-cut_facility-location_disparity-sum.pkl\n",
      "Error while loading facility-location_disparity-min_graph-cut_disparity-sum.pkl\n",
      "Error while loading disparity-min_disparity-sum_graph-cut_facility-location.pkl\n",
      "Error while loading facility-location_disparity-sum_disparity-min_graph-cut.pkl\n",
      "Error while loading disparity-min_facility-location_disparity-sum_graph-cut.pkl\n",
      "Error while loading disparity-sum_facility-location_disparity-min_graph-cut.pkl\n",
      "Error while loading facility-location_disparity-sum_graph-cut_disparity-min.pkl\n",
      "Error while loading disparity-sum_facility-location_graph-cut_disparity-min.pkl\n",
      "Error while loading disparity-min_facility-location_graph-cut_disparity-sum.pkl\n",
      "Error while loading disparity-min_graph-cut_disparity-sum_facility-location.pkl\n",
      "Error while loading facility-location_graph-cut_disparity-min_disparity-sum.pkl\n",
      "Error while loading disparity-min_disparity-sum_facility-location_graph-cut.pkl\n",
      "Error while loading facility-location_disparity-min_disparity-sum_graph-cut.pkl\n",
      "Error while loading disparity-sum_disparity-min_facility-location_graph-cut.pkl\n",
      "Error while loading facility-location_graph-cut_disparity-sum_disparity-min.pkl\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "for exp_data_file in dir_list:\n",
    "    data = None\n",
    "    if exp_data_file.endswith(\".pkl\"):\n",
    "        try:\n",
    "            with open(f\"./data/seq_submod_subset/permutation_subsets_4/{exp_data_file}\", \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "        except:\n",
    "            print(f\"Error while loading {exp_data_file}\")\n",
    "            continue\n",
    "    \n",
    "    if data is None:\n",
    "        print(\"Data is None!!\")\n",
    "    if model_name==\"LeNet\":\n",
    "        model = LeNet()\n",
    "    elif model_name==\"resnet18\":\n",
    "        model = get_resent18_model()\n",
    "    elif  model_name==\"resnet101\":\n",
    "        model = get_resent101_model()\n",
    "\n",
    "    model = model.to(device=device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name==\"SGD_ann\":\n",
    "        optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    elif optimizer_name==\"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Train loop\n",
    "        if epoch==0:\n",
    "            sub_dataset = SubDataset(indices=data[0], dataset=train_dataset)\n",
    "            subset_train_dataloader = DataLoader(sub_dataset, batch_size=64, shuffle=True)\n",
    "        elif epoch==50:\n",
    "            sub_dataset = SubDataset(indices=data[1], dataset=train_dataset)\n",
    "            subset_train_dataloader = DataLoader(sub_dataset, batch_size=64, shuffle=True)\n",
    "        elif epoch==100:\n",
    "            sub_dataset = SubDataset(indices=data[2], dataset=train_dataset)\n",
    "            subset_train_dataloader = DataLoader(sub_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "            \n",
    "        for images, labels in subset_train_dataloader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            if optimizer_name==\"SGD_ann\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()  # Update model weights\n",
    "                lr_scheduler.step()     \n",
    "            elif optimizer_name==\"adam\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    time_taken = time.time() - start_time    \n",
    "    print(\"--- %s seconds ---\" % (time_taken))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "\n",
    "    x = range(epochs)\n",
    "\n",
    "    # Plot the accuracies\n",
    "    plt.clf()\n",
    "    plt.plot(x, accuracy_list)\n",
    "\n",
    "    # Customize the plot (optional)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{exp_data_file.split('.')[0]} Accuracy Plot\")\n",
    "\n",
    "    # Display the plot\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"./results/seq/plots/{exp_data_file.split('.')[0]}\")\n",
    "\n",
    "    with open(f\"./results/seq/accuracies/{exp_data_file.split('.')[0]}\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(accuracy_list)\n",
    "        \n",
    "    res[exp_data_file.split('.')[0]] = accuracy_list\n",
    "        \n",
    "    with open(f\"./results/seq/accuracies.pkl\", \"wb\") as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mclf()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, data \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(data, label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.clf()\n",
    "for label, data in res.items():\n",
    "    plt.plot(data, label=label)\n",
    "\n",
    "plt.tick_params(bottom=False, labelbottom=False)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"All Accuracies\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./results/seq/accuracies.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submodlib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
