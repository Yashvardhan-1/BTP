{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1c75884f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "#from torchvision.models.resnet import ResNet18_Weights\n",
    "import pickle\n",
    "import random\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda:4\" # change the available gpu number\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 datasets\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "subset_fraction = 0.1\n",
    "epochs = 1\n",
    "model_name = \"LeNet\"\n",
    "optimizer_name = \"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.LeNet_model import LeNet\n",
    "from models.resent_models import get_resent101_model, get_resent18_model\n",
    "\n",
    "model = LeNet(out_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.milo.subset_sampler import RandomSubsetSampler\n",
    "from utils.milo.subset_dataset import SubDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimizer_name==\"SGD_ann\":\n",
    "    optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "elif optimizer_name==\"adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/ganesh/namitha/yash/BTP/neural-submod\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_list = [\"facility-location\", \"disparity-min\",  \"disparity-sum\", \"graph-cut\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = \"./data/seq_submod_subset/permutation_subsets_2/\"\n",
    "dir_list = os.listdir(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/seq_submod_subset/permutation_subsets_2/facility-location_disparity-sum.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disparity-min_graph-cut.pkl', 'graph-cut_facility-location.pkl', 'facility-location_graph-cut.pkl', 'disparity-sum_facility-location.pkl', 'disparity-min_disparity-sum.pkl', 'disparity-sum_disparity-min.pkl', 'facility-location_disparity-sum.pkl', 'graph-cut_disparity-sum.pkl', 'disparity-sum_graph-cut.pkl', 'graph-cut_disparity-min.pkl', 'facility-location_disparity-min.pkl', 'disparity-min_facility-location.pkl']\n"
     ]
    }
   ],
   "source": [
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for order in permutations(func_list):\n",
    "    per_func_list = list(order)\n",
    "    file1 = per_func_list[0]+\"_\"+per_func_list[1]\n",
    "    file2 = per_func_list[2]+\"_\"+per_func_list[3]\n",
    "\n",
    "    data1 = None\n",
    "    data2 = None\n",
    "\n",
    "    if f\"{file1}.pkl\" in dir_list and f\"{file2}.pkl\" in dir_list:\n",
    "        try:\n",
    "            with open(f\"./data/seq_submod_subset/permutation_subsets_2/{file1}.pkl\", \"rb\") as f:\n",
    "                data1 = pickle.load(f)\n",
    "            with open(f\"./data/seq_submod_subset/permutation_subsets_2/{file2}.pkl\", \"rb\") as f:\n",
    "                data2 = pickle.load(f)\n",
    "        except:\n",
    "            print(\"Error while loading the data\")\n",
    "            continue\n",
    "        \n",
    "    if data1 is None or data2 is None:\n",
    "        print(\"data is None\")\n",
    "        continue \n",
    "\n",
    "    if model_name==\"LeNet\":\n",
    "        model = LeNet()\n",
    "    elif model_name==\"resnet18\":\n",
    "        model = get_resent18_model()\n",
    "    elif  model_name==\"resnet101\":\n",
    "        model = get_resent101_model()\n",
    "\n",
    "    model = model.to(device=device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name==\"SGD_ann\":\n",
    "        optimizer = SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "        lr_scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    elif optimizer_name==\"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Train loop\n",
    "        if epoch==0:\n",
    "            sub_dataset = SubDataset(indices=list(set(data1[0]) | set(data2[0])), dataset=train_dataset)\n",
    "            subset_train_dataloader = DataLoader(sub_dataset, batch_size=64, shuffle=True)\n",
    "        elif epoch==20:\n",
    "            sub_dataset = SubDataset(indices=list(set(data1[1]) | set(data2[1])), dataset=train_dataset)\n",
    "            subset_train_dataloader = DataLoader(sub_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "            \n",
    "        for images, labels in subset_train_dataloader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Backward pass and update weights\n",
    "            if optimizer_name==\"SGD_ann\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()  # Update model weights\n",
    "                lr_scheduler.step()     \n",
    "            elif optimizer_name==\"adam\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    time_taken = time.time() - start_time    \n",
    "    print(\"--- %s seconds ---\" % (time_taken))\n",
    "\n",
    "    # Evaluate on test set\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "\n",
    "    x = range(epochs)\n",
    "\n",
    "    # Plot the accuracies\n",
    "    plt.clf()\n",
    "    plt.plot(x, accuracy_list)\n",
    "\n",
    "    # Customize the plot (optional)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{file1}_{file2} Accuracy Plot\")\n",
    "\n",
    "    # Display the plot\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"./results/para/plots/{file1}_{file2}.png\")\n",
    "    \n",
    "    res[f\"{file1}_{file2}\"] = accuracy_list\n",
    "\n",
    "    print(accuracy_list)\n",
    "\n",
    "    with open(f\"./results/para/accuracies/{file1}_{file2}.txt\", \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(accuracy_list)\n",
    "        \n",
    "    with open(f\"./results/para/accuracies.pkl\", \"wb\") as f:\n",
    "        pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for label, data in res.items():\n",
    "    plt.plot(data, label=label)\n",
    "\n",
    "plt.tick_params(bottom=False, labelbottom=False)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"All Accuracies\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./results/para/accuracies.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submodlib-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
